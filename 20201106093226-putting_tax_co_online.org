#+title: putting tax.co online
* BLOCKED asking Angelica about reimbursement
* TODO what permissions does a web visitor have?
* TODO be sure I don't have FTP (port 21) open
* TODO internalize
** keep tax.co.web branches main and online (mostly) synced
* TODO blocking
** DONE WHy won't it build?
*** stdout.txt shows `make` not even trying.
    jeff@jbb-dell:~/of/tax.co/online/users/ue01af9d7df5e7e220c6ef60e400a6683]$ cat stdout.txt
    make: Nothing to be done for 'bash/run-makefile.py'.
    cat /mnt/tax_co/users/ue01af9d7df5e7e220c6ef60e400a6683/config/shell.json
    {"user_email": "1@donde.net", "subsample": 1000, "strategy": "detail", "regime_year": 2019}PYTHONPATH='.' python3 python/build/rate_input_test.py /mnt/tax_co/users/ue01af9d7df5e7e220c6ef60e400a6683/config/shell.json
** DONE What put user_email after user?
   uniquify was doing it, because grouping and aggregating puts the grouping variables first.
** DONE [[id:927559bb-036d-49fc-9fdd-024745fb941b][What figures out which one to run next?]]
** DONE why is user 1's getting deleted early?
   I thought tax.co was deleting the data, but no, it was tax.co.web,
   upon submitting the new request. Duh.
** DONE [[id:cd69f027-d73b-4d3d-be8f-bf0a6c7d90e7][test the program (by hand)]]
** DONE [[file:../tech/20210511124423-give_appuser_apache_privileges.org][giving Apache privileges to a Docker user]]
** DONE [[id:d002eec7-7fe7-4130-8423-88a6408c46ed][run tax.co as a cron job]]
** TODO [[id:d3900b4b-c97d-4bd5-a898-06281df235be][email results to user]]
** TODO [[id:c04905da-d0fe-4eac-af96-c2a3bcc03649][hide my email credentials unavailable]]
** DONE [[file:../tech/20210419160403-django_security_things_to_do_before_production.org][Django security, things to do before production]]
** if sp . returncode == 0:
        # TODO : `make` returns 0 even when from my point of view it didn't work, so this is unreliable.
        lib . mutate (
            requests_path,
            lambda reqs: lib . mark_complete (
                user_hash, reqs ) )
** [[id:1c9cef73-d495-4735-a789-2daf051c9beb][show Makefile errors if build fails]]
* TODO important
** DONE correct low-ceiling error in most.csv
   The top income tax rate for most income ended at a finite income level; it shouldn't. Now it doesn't.
   Daniel helped me find the error.
   Comparing to the old code (still the latest commit in the main branch),
   was able to see what the rates should have been.
** [[id:b7595065-bed2-4703-875a-7077a1ee72b2][generate pictures]]
** [[id:52884b96-5c15-42d9-a55f-32c013934ffb][enable .xlsx or .csv upload]]
* TODO use Aaron's code
** updating it
  git fetch origin pull/1/head && git checkout FETCH_HEAD
** how to run it
   docker-compose -f docker-compose-deploy.yml up --build
   http://localhost:8080/run_make/ingest_full_spec
** questions/comments for him
  I see dependencies on a few URLs.
  Don't really need every template changed; users aren't expected to visit most of them, just "ingest" and "thank".
* TODO easy
** random-seed tax.co
   because some jitter is used to compute income quantiles
** switch mount points to ~
   so that cd brings me there
** [[id:9c7e0481-328c-46ed-82ab-57759d24f8b8][Move all file definitions to dedicated files.py files]].
** make code more understandable
*** in commands.sh, rather than hardcode paths, define commented variables
    so the user knows to redefine them
    maybe better, read paths.json
*** make the users folder for quien@donde.net
*** make the default run-makefile command not use "jeff" but instead the hash for quien@donde.net
*** change the email address in users/exmaple/
*** in users/example, include vat_by_*.csv
*** what about tax.co.web/secret/ ?
    and SECRET_KEY in ebdjango settings?
* TODO in tax.co
** DONE Docker: change time zone to COT.
** DONE take .json inputs
*** make a new branch, "online"
*** add a new Makefile argument
    the config.json file
*** define usage in a comment of make.py
*** use config.json to define the standard Makefile arguments
    subsample=$1
    regime_year=$regime_year
    strategy=$strategy
*** merge common* programs
    I don't need to split the command line and the repl any more.
    And I need the definition of common.valid_* to work from both contexts.
*** test that it ingested properly
    I can first leave all the Makefile recipes that use the command line-defined arguments in place. They don't need to use config.json yet. Instead just add a recipe that calls a new .py file that reads the json, defines some variables, and prints them to screen.
*** rewrite extant Makefile recipes
    to use config.json and ignore the earlier Makefile arguments
*** change these files to only use the full sample
**** DONE python/build/ss_functions_test.py
**** DONE python/build/people_2_buildings.py
**** DONE python/build/purchases/input_test.py
**** DONE python/regime/r2018_test.py
**** DONE python/build/buildings_test.py
*** add a config param: VAT schedules (spreadsheet)
*** add tests for spreadsheet valididty
** TODO ? in lib.py, canonicalize on read routine, write routine
*** the idea
    This prevents needing to canonicalize everywhere.
    There might not yet be a write routine; if so make it.
*** the problem
    I might mutate reqs in a function, then pass it as an argument to another function, with no read or write step. Therefore the receiving function still has to canonicalize.
** DONE what figures out which one to run next?
   :PROPERTIES:
   :ID:       927559bb-036d-49fc-9fdd-024745fb941b
   :END:
*** the problem
    In tax.co, python.requests.main is called with a particular user's folder as an argument. Therefore it must get called from something else that figures out which user to apply it to.
*** verify try_to_advance_request_queue and advance_request_queue don't need their user_hash arguments
*** make try_to_advance_request_queue depend not on the user shell.json argument
**** Why this is a good idea
     Here are the only routines in python.requests that depend on the user specified by shell.json:
 lib.py:
   this_request()
 main.py:
   add-to-temp-queue (section, not function)
     It calls
       lib.this_request().
   try-to-advance-user
     It passes c.user to
       main.try_to_advance_request_queue().
** DONE Can I delete the "max_runtime_minutes" parameter?
   I see nowhwere it's used, and I don't see how I'd use it.
** csv-dynamic income tax regimes
   :PROPERTIES:
   :ID:       1d3000ca-5771-4495-9632-099b606c277c
   :END:
*** only for regime 2019
*** Haskell: share libraries
*** generate working Python
*** turn CSV into a [Formula]
**** validateTable should be called in tableToMoneyBrackets
     not in csvToPython
*** clear out those ", proposed" variables
*** build an executable, callable from shell, with command-line args
    for translating a .csv file
*** duplicate the hard-coded functions with some .csv-dynamic ones
**** make the .csv files' location a config param
     That location should have each of the files needed --
     most_income.csv, dividend.csv, etc.
**** keep .csv and generated .py under python/csv-dynamic
     Some of the .csv can be permanent.
**** build, execute a dynamic import statement
     It can be executed with `exec`,
       which is type String -> IO ().
     It imports the needed .csv-generated .py files.
     It is executed in python/regime/r2019.py.
*** test that they give the same answers
*** then delete the hard-coded 2019 functions
*** TODO Ponder: Why was this so much harder than expected?
** TODO generate pictures
   :PROPERTIES:
   :ID:       b7595065-bed2-4703-875a-7077a1ee72b2
   :END:
*** decide which to draw
*** code drawing them
*** patch that into the website
** TODO ? Makefile must catch all changes
   :PROPERTIES:
   :ID:       306f0e24-363e-4a61-99b3-0ef3028c57f1
   :END:
*** details
   Inc. changes to the user-supplied .csv files,
   on which (only?) r2019 depends.
*** recursive import tracing
    Can I encode the imports of a program as a recipe that does nothing,
    to ensure that it is re-run whenever any of those imports changes,
    without having to list dependencies of dependencies in each recipe
    that actually does something?
*** BLOCKED add Haskell files
    Adding them to make/deps is easy.
    The hard part is using them in make/build.
** TODO ? Makefile: smart within user
   It won't recreate data products unnecessarily when I'm testing.
** solve memory, time constraints, cron job
   :PROPERTIES:
   :ID:       c3c33450-e196-4116-be1e-7b253bc68391
   :END:
*** DONE choose optimal wait
    Promise to respond within 2 hours,
    and to hold the results for at least 1 hour after making them.
    If space for 10 users, then actually the response will always come in at most 100 minutes, and the data will stay for at least 100 minutes.
    If no new users bump the space, they might stay longer.
*** DONE compute hash of email address
    This will be treated like a user name.
*** DONE NEXT all* output should go to a specific user's folder
    * except the subsample, which is slow and extremely initial
**** places to change to_csv
     report/overview.py
     build/output_io
*** DONE add new user to db of requests
**** sort  : time of request
**** field : hash of email
**** field : time of requests
**** field : time of results
     often missing
*** DONE maintain a .json file of spacetime params
    data/constraints-time-memory.json
*** DONE The program will have to use `dh` from the shell.
*** DONE incorporate requests/test.py into Makefile
*** TODO find appropriate constraints for serving full sample
    Can only be done from EC2.
** test the program (by hand)
   :PROPERTIES:
   :ID:       cd69f027-d73b-4d3d-be8f-bf0a6c7d90e7
   :END:
*** DONE starting one when memory is full and time's not up
*** DONE try starting one when memory is full and time's up
*** DONE try starting one when more than one are pending
    make sure the right one started
*** DONE try starting one "try-to-advance" while another is running
    :PROPERTIES:
    :ID:       f3149cb1-d95d-4dc7-ab07-27cf08eafa1c
    :END:
    Because one cron job could do this to an earlier one.
*** DONE try running "try-to-advance-queue" giving the wrong user config
    It shouldn't make a difference.
*** DONE what happens if a user has two completed requests in requests.csv?
    Nothing bad happens -- they get collapsed into one before tax.co tries to delete the corresponding folder.
    (Otherwise it would bork the next time it tries to delete the same data, because it won't be there.)
*** DONE collect other tests here
* TODO in Django
** DONE solve Django bug: filesystem not always written to
*** forum question
    https://forum.djangoproject.com/t/view-only-sometimes-writes-to-filesystem/6799
*** where the bug happens
The last commit that works:
  89a231c3bda51c3e245e1991a57b1b3f814cd3be
The first that fails:
  cb0e71e9ee3b3f9253cf2c21e376c7759e3ef6f0
** DONE send data to tax.co
*** create folder with name = hash of user email
*** insert json spec
*** rename ingest_spec -> ingest_json
    and move it to "examples",
    and then start on "ingest_spec", which ingests both json and tables
*** factor out functions from ingest_json
    The one that makes the user folder if needed,
    and writes the json config data to it.
*** insert spreadsheets
**** in upload_multiple.html, read list of table names
     from the calling Django view.
**** make spreadsheets in tax.co shareable
     Move them to to-serve/,
     and simlink their original locations to the new ones.
     Then run tax.co to make sure they work.
**** configure Apache to find tax.co spreadsheets
**** Allow download of default spreadsheets.
**** handle the case of an invalid spec form
     in ingest_full_spec
     The trick was to populate  the optional "choices" fields of the Model elements.
**** rearrange file tree
     I want the user to have free access to tax.co,
     but not to any secret keys in, say, web/.
**** use symlinks for files not uploaded.
     It could be that the user's folder always has a file for every uploadable table, but that in the event that they don't upload it, that file is a symlink.
     This simplifies the config file -- no need to indicate where the files are, becuase they're always there -- and doesn't have much effect + or - on the simplicity of the code that puts the files there.
**** remove some now-obsolete shell.json params
 "vat_by_coicop"         : "data/vat/vat-by-coicop.csv",
 "vat_by_capitulo_c"     : "data/vat/vat-by-capitulo-c.csv",
 (and change all the code that used to depend on those,
 to use the symlinks instead)
**** ? move the spec to a subfolder
     of the user folder called spec/
     where "the spec" includes all uploaded tables too.
**** handle the case that an uploaded file already exists
** DONE rewrite link.sh to copy ports.conf in addition to apache2.conf
** DONE rename /mnt/web -> /mnt/django
** TODO ? split email address from other details
   (When I first tried fixing this problem something went wrong I didn't understand.)
   It's mandatory and obvious, whereas the rest are optional and esoteric.
     Therefore they deserve a preamble, but it doesn't.
** TODO determine whether, when to run
*** CANCELED change import path to see the db functions
    Hard to do. Instead, call tax.co/python/requests from tax.co.web
*** DONE split tax.co/python/requests.py into lib, tests, main
*** DONE on each run of the view: add request to tax.co/data/requests.csv
*** DONE the code expects vat_by_c*, not vat-by-c*
    That is, underscores, not dashes.
    So change all the filenames accordingly.
    Also change the READMEs (plural) in data/vat
*** DONE NEXT get try-to-advance to work in the repl
*** DONE NEXT get try-to-advance to work from the shell
**** IMPORTANT: DON"T MESS WITH tax.co/master
     because tax.co/web has unsaved changes,
     some for debugging and maybe some that fix bugs
**** do it from within tax_web docker container
**** may need to os.chdir to /mnt/tax_co
     once running python from a different python
*** DONE bugfix: delete the oldest *extant* user
    :PROPERTIES:
    :ID:       51d7d5fb-baa0-4558-8bdf-463b6d77f902
    :END:
    Call it liek this
    (but change the value "4" to whatever is appropriate).

    PYTHONPATH=/mnt/tax_co/ python3         \
      /mnt/tax_co/python/requests/main.py   \
      /mnt/tax_co/users/1/config/shell.json \
      try-to-advance
*** DONE fix: view currently doesn't trigger add-to-requests
    and yet this works from anywhere in the shell (in the docker container):
  PYTHONPATH=/mnt/tax_co/                                               \
  python3                                                               \
  /mnt/tax_co/python/requests/main.py                                   \
  /mnt/tax_co/users/972411cda1a01ae85f6c36b1b68118c3/config/shell.json  \
  add-to-queue
*** DONE clean requests/main.py
  Change _file and _folder to _path.
    This makes searching easier.

  In advance_request_queue, don't redefine tax_root.
*** DONE change os.system calls to subprocess.run calls
    can model on tax.co/python/requests/main.py
*** DONE how to advance requests (on cron's time)
**** THINKING: unused functions
***** delete_oldest_user_folder
***** gb_used
***** memory_permits_another_run
***** delete_oldest_request
***** at_least_one_is_old
***** unexecuted_requests_exist
**** the work
     See if unexecuted requests exist.
     If so, see if it can be run yet.
     If there's room for another already, run the oldest unexecuted request.
     If there's no room, but some request is old enough to be deleted,
     then delete it from requests.csv and users/,
     and then run the oldest unexecuted request.
     Once the request has executed, mark it complete.
*** DONE ! introduce a memory lock
**** the filelock library seems good
     https://pypi.org/project/filelock/
     https://stackoverflow.com/a/498505/916142
**** strategy
***** temporarily hold new requests in a briefly-accessed file
      Keep a file next to requests.csv called requests.new.csv.
      Each time a user submits a request,
      add it to requests.new.csv, rather than requests.csv.
      Each time the cron job runs, it transfers from requests.new.csv to requests.csv.
      The advantage of this is that the file is never needed for very long, so no process will meaningfully block another.
***** only the cron job accesses requests.csv
**** DONE stale
***** why
     Otherwise one instance of the cron job could clobber another,
     or a user request could be missed
     because the cron job held an earlier copy of requests.csv.
***** if I were to DIY it
****** To lock a file,
       save a file of the same name with ".lock" appended. Optionally, write in the file the reason it's locked.
****** To unlock a file,
       delete the lock. But don't do that unless the lock is yours.
****** To wait on a file
       See if the file is locked.
       If so, wait a given (as an argument) number of seconds.
*** DONE resubmission problems
    :PROPERTIES:
    :ID:       6d78fc5f-9958-4b28-9ad5-b74e20c7b12a
    :END:
**** DONE If someone resubmits, delete earlier config, but keep earlier submission date.
***** to test
****** From zero data: Create users 1 and 2.
****** Give user 1 a bogus spreadsheet.
****** Resubmit user 1.
****** Verify the bogus spreadsheet is gone.
****** DONE ? Run try-to-advance, verify that user 1 is the one advanced.
**** DONE Don't bork if someone resubmits when they already have a completed request.
***** to test
****** submit user 1 request, then user 2 request
****** process user 1 request
****** submit a new user 1 request
****** process user 2 request
****** process user 1 request
****** make sure nothing borked
**** DONE add disclaimer to webpage
     Explain that that's what happens.
*** DONE read shell variables from a .json file
    Using [[file:../tech/20210414161239-jq_shell_command.org][jq (shell command)]].
    See commands.sh (in the tax.co.web repo)
** TODO enable .xlsx or .csv upload
   :PROPERTIES:
   :ID:       52884b96-5c15-42d9-a55f-32c013934ffb
   :END:
*** keep original filename extensions
    Currently the symlink always ends in .csv,
    even though the file itself might end in .xlsx.
** TODO move all file definitions to dedicated files.py files
   :PROPERTIES:
   :ID:       9c7e0481-328c-46ed-82ab-57759d24f8b8
   :END:
   In both web/../views/ and tax/../request/
   find . -name "*.py" -print0 | xargs -0 grep "/" --color -l | grep -v "studies/"
*** more places?
    in ~/of/webapp/django/run_make/views/lib.py,
    in the function append_request_to_db().
    Also in tax.co/python/requests/.
*** use tax.co.web/paths.json
    commands.sh already uses it,
    but Python doesn't.
** TODO show Makefile errors if build fails
   :PROPERTIES:
   :ID:       1c9cef73-d495-4735-a789-2daf051c9beb
   :END:
*** convey exit status to webapp
*** write error to a file
*** find, display that error file in the webapp
** TODO email results to user
   :PROPERTIES:
   :ID:       d3900b4b-c97d-4bd5-a898-06281df235be
   :END:
*** DONE email a hello.txt file
**** put secrets in a tax_co/secret folder
     files named "email address", "password"
*** DONE make a .zip file with all the logs, and the config file
**** do user/../std* not get made any more?
     If so, redirect run-makefile to write there, not in tax.co/make-logs/
**** they include view.std* and std*
**** send config file also, to know user's email & hash
**** use `zipfile` library (builtin)
*** separately send the .xlsx
** TODO ? email addresses with strange characters
   Django does not accept them.
   Are they important?
* in Apache
** ? In Docker image, customize further [[id:dcc41642-ba24-45b8-bf55-daf08d7f701e][for Apache]] and [[file:../tech/20201014163254-wsgi.org][wsgi]]
** DONE bug-2021-04-19-permission-denied-on-browser-serving-locally
   Once that's solved, remember to uncomment the portion of ports.conf that allows serving online.
*** ? Give up
    Don't run locally.
*** ? COMPARE: This branch works.
    bug-2021-04-19-SOLVING-works-locally
*** FALSE suspicion: sim.jefbrown.net is confusing it
    In the branch that fails,
    even when I delete sim.jefbrown.net in
      ebdjango/settings.py
    in the definition of
      ALLOWED_HOSTS
    it still doesn't work.
** DONE cannot download models from myapp/run_make/ingest_full_spec
* integrate tax.co and the web app
  :PROPERTIES:
  :ID:       f94012e6-e4ad-4e3a-bd68-d3a82fb165de
  :END:
** DONE user downloads .csv
** DONE user uploads .csv, inputs .json
** DONE tax.co finds user input
** TODO email results to user as soon as they exist
* TODO hide my email credentials
  :PROPERTIES:
  :ID:       c04905da-d0fe-4eac-af96-c2a3bcc03649
  :END:
  They're not in the repo, but they'll be on the server,
  so they shouldn't be in tax_co, which is publicly readable.
* DONE run tax.co as a cron job
  :PROPERTIES:
  :ID:       d002eec7-7fe7-4130-8423-88a6408c46ed
  :END:
** verify that the command in the script works when entered by hand
** Do I have to start tax.co.web as root to serve?
   No.
** Bug, solved: The cron script is not executing.
*** the python3 call: works
    :PROPERTIES:
    :ID:       7deaf598-7d46-4ffa-870e-f99832dd58c7
    :END:
    PYTHONPATH=/mnt/tax_co:$PYTHONPATH python3 python/requests/main.py config/config.json try-to-advance-queue
*** running tax_co_cron.sh by hand works
    :PROPERTIES:
    :ID:       d37c67b8-408e-404d-abf4-b8c9747561b2
    :END:
*** ownserhip is weird
    Everything touched by either [[id:7deaf598-7d46-4ffa-870e-f99832dd58c7][the python3 call]] or [[id:d37c67b8-408e-404d-abf4-b8c9747561b2][running tax_co_cron.sh by hand]] is owned on the native EC2 system by
       user ubuntu, group systemd-coredump
    and in the Docker container by
       user appuser, group appuser
    whereas for other things the group is
       ubuntu in the native EC2 system
       1000 in the Docker container
*** whoami.sh leads cron to write "appuser"
*** Never had to [[file:../tech/20210513113219-python_script_works_unless_run_by_cron.org][ask the internets]].
*** Verify the same happens on EC2
* DONE choose a [[file:../tech/20210419112140-web_server.org][web server]]
* [[file:20210419112845-tax_co_web_setting_up_an_ec2_instance_to_serve.org][deploy tax.co.web over EC2]]
* TODO ponder
** Keep a db of requests?
   It seems like the "right" thing to do,
   but at the same time it's work for no obvious immediate gain.
** Cache results: hard problem
*** Hash each submitted configuration
    Based on tax config spec but not email address,
    so that if two people submit the same request,
    it'll be obvious.
*** Keep a db matching request hashes to (requests and) data products.
*** The Makefile recipes are for simlinks.
    Each request (a set greater than each hash-equivalent request)
    lives in its own folder. The Makefile creates simlinks from that folder
    to the "data products" folder.
*** When a request is made,
    the python code looks up whether
** Ponder: idle user time, parallelism
   Should the website pause while the model is computed?
