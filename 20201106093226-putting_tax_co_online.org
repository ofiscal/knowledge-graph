#+title: putting tax.co online
* TODO next: [[id:e20adb08-6439-484c-875c-1fb822f66634][determine whether, when to run]]
* TODO in tax.co
** DONE take .json inputs
*** make a new branch, "online"
*** add a new Makefile argument
    the config.json file
*** define usage in a comment of make.py
*** use config.json to define the standard Makefile arguments
    subsample=$1
    regime_year=$regime_year
    strategy=$strategy
*** merge common* programs
    I don't need to split the command line and the repl any more.
    And I need the definition of common.valid_* to work from both contexts.
*** test that it ingested properly
    I can first leave all the Makefile recipes that use the command line-defined arguments in place. They don't need to use config.json yet. Instead just add a recipe that calls a new .py file that reads the json, defines some variables, and prints them to screen.
*** rewrite extant Makefile recipes
    to use config.json and ignore the earlier Makefile arguments
*** change these files to only use the full sample
**** DONE python/build/ss_functions_test.py
**** DONE python/build/people_2_buildings.py
**** DONE python/build/purchases/input_test.py
**** DONE python/regime/r2018_test.py
**** DONE python/build/buildings_test.py
*** add a config param: VAT schedules (spreadsheet)
*** add tests for spreadsheet valididty
*** accept either .xlsx or .csv format
** csv-dynamic income tax regimes
   :PROPERTIES:
   :ID:       1d3000ca-5771-4495-9632-099b606c277c
   :END:
*** only for regime 2019
*** Haskell: share libraries
*** generate working Python
*** turn CSV into a [Formula]
**** validateTable should be called in tableToMoneyBrackets
     not in csvToPython
*** clear out those ", proposed" variables
*** build an executable, callable from shell, with command-line args
    for translating a .csv file
*** duplicate the hard-coded functions with some .csv-dynamic ones
**** make the .csv files' location a config param
     That location should have each of the files needed --
     most_income.csv, dividend.csv, etc.
**** keep .csv and generated .py under python/csv-dynamic
     Some of the .csv can be permanent.
**** build, execute a dynamic import statement
     It can be executed with `exec`,
       which is type String -> IO ().
     It imports the needed .csv-generated .py files.
     It is executed in python/regime/r2019.py.
*** test that they give the same answers
*** then delete the hard-coded 2019 functions
*** TODO Ponder: Why was this so much harder than expected?
** TODO generate pictures
*** decide which to draw
*** code drawing them
*** patch that into the website
** Makefile must catch all changes
   :PROPERTIES:
   :ID:       306f0e24-363e-4a61-99b3-0ef3028c57f1
   :END:
*** details
   Inc. changes to the user-supplied .csv files,
   on which (only?) r2019 depends.
*** recursive import tracing
    Can I encode the imports of a program as a recipe that does nothing,
    to ensure that it is re-run whenever any of those imports changes,
    without having to list dependencies of dependencies in each recipe
    that actually does something?
*** BLOCKED add Haskell files
    Adding them to make/deps is easy.
    The hard part is using them in make/build.
** solve memory, time constraints, cron job
   :PROPERTIES:
   :ID:       c3c33450-e196-4116-be1e-7b253bc68391
   :END:
*** choose optimal wait
    Promise to respond within 2 hours,
    and to hold the results for at least 1 hour after making them.
    If space for 10 users, then actually the response will always come in at most 100 minutes, and the data will stay for at least 100 minutes.
    If no new users bump the space, they might stay longer.
*** compute hash of email address
    This will be treated like a user name.
*** NEXT all* output should go to a specific user's folder
    * except the subsample, which is slow and extremely initial
**** places to change to_csv
     report/overview.py
     build/output_io
*** add new user to db of requests
**** sort  : time of request
**** field : hash of email
**** field : time of requests
**** field : time of results
     often missing
*** maintain a .json file of spacetime params
    data/constraints-time-memory.json
*** The program will have to use `dh` from the shell.
*** Makefile: smart within user
    It won't recreate data products unnecessarily when I'm testing.
*** incorporate requests_test.py into Makefile
* TODO in Django
** DONE solve Django bug: filesystem not always written to
*** forum question
    https://forum.djangoproject.com/t/view-only-sometimes-writes-to-filesystem/6799
*** where the bug happens
The last commit that works:
  89a231c3bda51c3e245e1991a57b1b3f814cd3be
The first that fails:
  cb0e71e9ee3b3f9253cf2c21e376c7759e3ef6f0
** DONE send data to tax.co
*** create folder with name = hash of user email
*** insert json spec
*** rename ingest_spec -> ingest_json
    and move it to "examples",
    and then start on "ingest_spec", which ingests both json and tables
*** factor out functions from ingest_json
    The one that makes the user folder if needed,
    and writes the json config data to it.
*** insert spreadsheets
**** in upload_multiple.html, read list of table names
     from the calling Django view.
**** make spreadsheets in tax.co shareable
     Move them to to-serve/,
     and simlink their original locations to the new ones.
     Then run tax.co to make sure they work.
**** configure Apache to find tax.co spreadsheets
**** Allow download of default spreadsheets.
**** handle the case of an invalid spec form
     in ingest_full_spec
     The trick was to populate  the optional "choices" fields of the Model elements.
**** rearrange file tree
     I want the user to have free access to tax.co,
     but not to any secret keys in, say, web/.
**** use symlinks for files not uploaded.
     It could be that the user's folder always has a file for every uploadable table, but that in the event that they don't upload it, that file is a symlink.
     This simplifies the config file -- no need to indicate where the files are, becuase they're always there -- and doesn't have much effect + or - on the simplicity of the code that puts the files there.
**** remove some now-obsolete shell.json params
 "vat_by_coicop"         : "data/vat/vat-by-coicop.csv",
 "vat_by_capitulo_c"     : "data/vat/vat-by-capitulo-c.csv",
 (and change all the code that used to depend on those,
 to use the symlinks instead)
**** ? move the spec to a subfolder
     of the user folder called spec/
     where "the spec" includes all uploaded tables too.
**** handle the case that an uploaded file already exists
** TODO ? split email address from other details
   (When I first tried fixing this problem something went wrong I didn't understand.)
   It's mandatory and obvious, whereas the rest are optional and esoteric.
     Therefore they deserve a preamble, but it doesn't.
** TODO determine whether, when to run
   :PROPERTIES:
   :ID:       e20adb08-6439-484c-875c-1fb822f66634
   :END:
*** CANCELED change import path to see the db functions
    Hard to do. Instead, call tax.co/python/requests from tax.co.web
*** DONE split tax.co/python/requests.py into lib, tests, main
*** DONE on each run of the view: add request to tax.co/data/requests.csv
*** DONE the code expects vat_by_c*, not vat-by-c*
    That is, underscores, not dashes.
    So change all the filenames accordingly.
    Also change the READMEs (plural) in data/vat
*** DONE NEXT get try-to-advance to work in the repl
*** TODO NEXT get try-to-advance to work from the shell
**** BUG: [[file:../../.local/share/Trash/files/20210317174200-library_not_available_when_django_calls_python.org][library not available when Django pcalls Python]]
**** IMPORTANT: DON"T MESS WITH tax.co/master
     because tax.co/web has unsaved changes,
     some for debugging and maybe some that fix bugs
**** do it from within tax_web docker container
**** may need to os.chdir to /mnt/tax_co
     once running python from a different python
*** TODO bugfix: delete the oldest *extant* user
    Call it liek this
    (but change the value "4" to whatever is appropriate).

    PYTHONPATH=/mnt/tax_co/		    \
      /opt/conda/bin/python3.8		    \
      /mnt/tax_co/python/requests/main.py   \
      /mnt/tax_co/users/4/config/shell.json \
      try-to-advance
*** DONE fix: view currently doesn't trigger add-to-requests
    and yet this works from anywhere in the shell (in the docker container):
  PYTHONPATH=/mnt/tax_co/                                               \
  python3                                                               \
  /mnt/tax_co/python/requests/main.py                                   \
  /mnt/tax_co/users/972411cda1a01ae85f6c36b1b68118c3/config/shell.json  \
  add-to-queue
*** DONE clean requests/main.py
  Change _file and _folder to _path.
    This makes searching easier.

  In advance_request_queue, don't redefine tax_root.
*** TODO enable .xlsx upload
**** keep original filename extensions
     Currently the symlink always ends in .csv,
     even though the file itself might end in .xlsx.
*** TODO move all file definitions to dedicates files.py files
    In both web/../views/ and tax/../request/
*** DONE change os.system calls to subprocess.run calls
    can model on tax.co/python/requests/main.py
*** DONE how to advance requests (on cron's time)
**** THINKING: unused functions
***** delete_oldest_user_folder
***** gb_used
***** memory_permits_another_run
***** delete_oldest_request
***** at_least_one_is_old
***** unexecuted_requests_exist
**** the work
     See if unexecuted requests exist.
     If so, see if it can be run yet.
     If there's room for another already, run the oldest unexecuted request.
     If there's no room, but some request is old enough to be deleted,
     then delete it from requests.csv and users/,
     and then run the oldest unexecuted request.
     Once the request has executed, mark it complete.
*** DONE ! introduce a memory lock
**** the filelock library seems good
     https://pypi.org/project/filelock/
     https://stackoverflow.com/a/498505/916142
**** strategy
***** temporarily hold new requests in a briefly-accessed file
      Keep a file next to requests.csv called requests.new.csv.
      Each time a user submits a request,
      add it to requests.new.csv, rather than requests.csv.
      Each time the cron job runs, it transfers from requests.new.csv to requests.csv.
      The advantage of this is that the file is never needed for very long, so no process will meaningfully block another.
***** only the cron job accesses requests.csv
**** DONE stale
***** why
     Otherwise one instance of the cron job could clobber another,
     or a user request could be missed
     because the cron job held an earlier copy of requests.csv.
***** if I were to DIY it
****** To lock a file,
       save a file of the same name with ".lock" appended. Optionally, write in the file the reason it's locked.
****** To unlock a file,
       delete the lock. But don't do that unless the lock is yours.
****** To wait on a file
       See if the file is locked.
       If so, wait a given (as an argument) number of seconds.
*** TODO Assume they want the earlier request deleted.
    Provide a message explaining that's what happens.
*** TODO why did I need jq?
    The following expression reads a json file in which "paths" equals a sub-dictionary of terms,
    and turns each term into a shell variable.
      eval "$(jq -r '.paths | to_entries | .[] | .key + "=\"" + .value + "\""' < paths.json)"
    For instance, it expects an input file called paths.json like this:
      { "paths":
        { "tax_co"     : "/mnt/tax_co/",
          "tax_django" : "/mnt/web/",
          "apache"     : "/mnt/apache2"
        }
      }
*** TODO wart: some paths are defined in obscure places
    in ~/of/webapp/django/run_make/views/lib.py,
    in the function append_request_to_db().
    Also in tax.co/python/requests/.
** TODO show Makefile errors if build fails
   :PROPERTIES:
   :ID:       1c9cef73-d495-4735-a789-2daf051c9beb
   :END:
*** convey exit status to webapp
*** write error to a file
*** find, display that error file in the webapp
** TODO email URL to user
** TODO test email addresses with strange characters
** TODO calibrate time, memory constraints
   in data/constraints-time-memory.json
   To get the time right, will need to be using the server.
* ? In Docker image, customize further [[id:dcc41642-ba24-45b8-bf55-daf08d7f701e][for Apache]] and [[file:../tech/20201014163254-wsgi.org][wsgi]]
* TODO integrate tax.co and the web app
  :PROPERTIES:
  :ID:       f94012e6-e4ad-4e3a-bd68-d3a82fb165de
  :END:
** user downloads .csv
** user uploads .csv, inputs .json
** tax.co finds user input
** tax.co runs
** tax.co informs webapp if, when it finishes
** webapp emails user that it's ready, sends link
** webapp finds, presents tax.co output
* [[id:8949db07-95ac-44c8-9b7b-78d565c1943d][before production run, do web security stuff]]
* TODO find, deploy to a server
  :PROPERTIES:
  :ID:       6c1cd107-bffa-4ef2-879b-8adc1bbf942b
  :END:
** a cheap-looking bare-metal server rental
https://gthost.com/bare-metal-server/
** TODO BLOCKED : Can DTI serve the app?
*** case number 245936
*** my latest response to DTI
**** what I said in brief
     Who's in charge of this shit?
     Here's what the app does: _.
**** planned response
***** original, nixed
Justificación:
==============

  Varios me han dicho que tal vez DTI me puede servir un sitio web que estoy programando. (Todavía me falta tal vez un mes de trabajo en ello.)

Situación actual:
=================

  He preguntado muchas veces si lo pueden hacer, y descrito a muchos como funcionará la aplicación (a Claudia, Dario, Laura, y quien séa el que lea el correo mesaservicios_dti@javeriana.edu.co). Cada uno me ha dicho tengo que discutir con otra persona para resolver el pedido.

***** sent, 2021 02 02
Justificación:
==============

  Tengo entendido que la DTI me puede ayudar a albergar un sitio web interactivo que estoy programando para el Observatorio Fiscal de la Facultad de Ciencias Económicas y Administrativas.

Situación actual:
=================

  Me gustaría saber quién es la persona a cargo del hosting en el servidor de la universidad para poder proceder a implementar el proyecto que tenemos en el Observatorio.

Cambios esperados:
==================

  La microsimulación permitirá que alguien especifica parametros alternativos del sistema tributario -- la tasa del impuesto de renta, la tasa sobre dividendos, la IVA, etc. -- para ver como afectaría la economía. Un usuario especifica los parametros, y el sistema genera unas tablas y graficas. El usuario puede ver las graficas en el navegador, y puede descargar las tablas.

  La especificación del IVA es complejo, porque cada clase de bien puede cargar una tasa diferente. Para permitir que un usuario pueda especificar tasas diferentes para cada clase de bien, le da la opción de subir una tabla (.xslx) al sistema mientras escojan los otros parametros.

  El programa requiere alrededor de 50 GB. Cada vez que corre el modelo genera alrededor de 2.1GB de datos, y es multiusuario, así que tiene que guardar los datos de algunos usuarios a la misma vez. También empieza con alrededor de 10 GB de datos originales, de los cuales se genera los productos de datos espeficas para cada usuario. La mayoría de esos datos de orígen son la Encuesta Nacional de Presupuestos de Hogares, hecho por el DANE.

  La aplicación está hecho en un contenedor Docker, así que no necesita acceso al disco entero de la máquina anfitriona; solo necesita su propio directorio.

  El imagen Docker tendría un peso alrededor de 10 GB. (Eso ya he incluido en el anterior requisito de 100 GB.) Ese imagen incluye el servidor Apache; no tiene que usar otro servidor.

  Cada vez que el sistema corre la simulación, los resultados se guardan en el disco por un día. Una vez existen, envia un correo al usuario, dicidendoles que están listos para ver y dando un enlace donde encontrarlos. Si la porción de la memoria aporcionada para la aplicación está lleno cuando un usuario pide resultados, tienen que esperar hasta que los de algún usuario han sido borrados.

  Si algo falla durante el proceso, el sistema me enviará a mi un correo explicando el error. Por eso yo necesitaré acceso al contenedor Docker, y a la porción del disco que usa el contenedor Docker. No necesitaré acceso al sistema fuera de ese directorio.
*** who
**** mesaservicios-dti@javeriana.edu.co
**** Claudia Patricia Forero Rodriguez <cpforero@javeriana.edu.co>
    said to write to mesaservicios-dti
**** Dario Rivillas Ossa <drivilla@javeriana.edu.co>
**** Laura <Last name?> at DTI called me
     I explained the project, she said someone else would contact me.
     That was around Thursday Jan 28 or Friday 29.
*** what I want
  La microsimulación permite que alguien especifica parametros alternativas del sistema tributario -- la tasa del impuesto de renta, o la tasa sobre dividendos, o la IVA -- para ver como afectaría la economía. Un usuario especifica los parametros, y el sistema genera unos tablas y graficos. El usuario puede ver los graficos en el navegador, y puede descargar las tablas.

  La especificación del IVA es complejo, porque cada clase de bien puede cargar una tasa diferente. Para permitir que un usuario pueda especificar tasas diferentes para cada clase de bien, le da la opción de subir una tabla (.xslx) al sistema mientras escojan los otros parametros.

  El programa puede usar menos de 20 GB de memoría para almacenar los datos funamentales (la Encuesta Nacional de Presupuestos de Hogares, hecho por el DANE), los subidos por usuarios, y los creado por el sistema. Está hecho en un contenedor Docker, así que no necesita acceso al disco entero de la máquina anfitriona; solo necesita su propio directorio, lo cual puede empezar vacio.

  El imagen Docker tendría un peso alrededor de 10 GB. (Eso ya he incluido en el anterior requisito de 100 GB.) El imagen incluye el servidor Apache; no tiene que usar otro servidor.

  Si el imagen tuviera acceso a más memoria, podría usar menos capacidad computacional. Alacenaría los resultados de los usuarios, así que si alguien pide algo que ya ha simulado, no tendría que simularlo de nuevo. Si me dicen que puede usar, digamos, hasta 50 GB, entonces cuando está a punto de pasar ese nivel borraría los resultados más viejos hasta que puede mantenerse debajo de ese límite.
*** Claudia said to specify
    Sobre esta solicitud deben realizar un caso a la mesa de ayuda informan que ustedes tienen un programa donde están desarrollando una plataforma para  los servicios descrito en el correo, por favor en este correo ser especifico la parte técnica, como: el programa es multiusuario, como van hacer las conexiones, como va estar conectado el programa para que las persona ingresen los datos ejemplo por medio de  WEB? o como lo tiene pensado.
*** form that mesaservicios-dti sent me (filled)
  JUSTIFICACIÓN:

  Claudia Patricia Forero Rodriguez del DTI me dijo que yo podría entrar al sitio Servir-T. Quiero hacer eso para preguntar si la aplicación web que estoy desarrollando se puede servir de la javeriana, o si tendría que usar otro dominio web.

  SITUACIÓN ACTUAL:

  El sistema Servir-T no me reconoce. Estoy usando el mismo nombre de usuario (brown-j) y clave que me permiten entrar al correo Javeriana, a Teams, etc.

  ¿CUÁLES SON LOS CAMBIOS ESPERADOS?

  Espero o que el sistema me reconozca.
* TODO ponder
** Keep a db of requests?
   It seems like the "right" thing to do,
   but at the same time it's work for no obvious immediate gain.
** Cache results: hard problem
*** Hash each submitted configuration
    Based on tax config spec but not email address,
    so that if two people submit the same request,
    it'll be obvious.
*** Keep a db matching request hashes to (requests and) data products.
*** The Makefile recipes are for simlinks.
    Each request (a set greater than each hash-equivalent request)
    lives in its own folder. The Makefile creates simlinks from that folder
    to the "data products" folder.
*** When a request is made,
    the python code looks up whether
** Ponder: idle user time, parallelism
   Should the website pause while the model is computed?
